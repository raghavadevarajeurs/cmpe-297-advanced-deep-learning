{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio_Translation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpNcgIGu0qgm"
      },
      "source": [
        "## Speech to Text using huggingfaces and wav2vec2 \n",
        "\n",
        "Ref: https://towardsdatascience.com/building-nlp-web-apps-with-gradio-and-hugging-face-transformers-59ce8ab4a319\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhsD4ePixW_l"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT7nir1-th-R",
        "outputId": "b56f9a54-6d0c-41f0-e26c-265afcab286b"
      },
      "source": [
        "!pip install gradio -q\n",
        "!pip install wandb --upgrade -q\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-48LJjQxUXg"
      },
      "source": [
        "import gradio as gr\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from transformers import Wav2Vec2ForMaskedLM, Wav2Vec2Tokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "k-TWlDFpxdvZ",
        "outputId": "5f149dd1-a43f-4079-8c23-5be4e553da3c"
      },
      "source": [
        "import torch\n",
        "import wandb\n",
        "wandb.login()\n",
        "from tqdm import tqdm\n",
        "#wandb.init(project=\"Audio2Text\", entity=\"raghavadurs\", id=\"asr_5\")\n",
        "wandb.init(project=\"voice2text\", entity=\"sjsu-cmpe-258-musketeers\" ,id=\"asr_5\")\n",
        "config = wandb.config\n",
        "config.learning_rate = 0.01"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msjsu-cmpe-258-musketeers\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/sjsu-cmpe-258-musketeers/voice2text/runs/asr_5\" target=\"_blank\">asr_5</a></strong> to <a href=\"https://wandb.ai/sjsu-cmpe-258-musketeers/voice2text\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjTJxghmzDgt",
        "outputId": "541ac1d5-9f66-46e2-cade-cf0e6657113d"
      },
      "source": [
        "# load wav2vec2 tokenizer and model\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
        "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
        "model = Wav2Vec2ForMaskedLM.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
        "\n",
        "#define speech to text function\n",
        "def asr_transcribe(audio_file):\n",
        "  transcript = \"\"\n",
        "\n",
        "  # Stream over 20seconds chunks\n",
        "  stream = librosa.stream(\n",
        "      audio_file.name, block_length=20, frame_length=16000, hop_length=16000\n",
        "  )\n",
        "\n",
        "  for speech in stream:\n",
        "    if len(speech.shape) > 1:\n",
        "      speech = speech[:, 0] + speech[:, 1]\n",
        "\n",
        "    input_values = tokenizer(speech, return_tensors=\"pt\").input_values\n",
        "    logits = model(input_values).logits\n",
        "\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
        "    transcript += transcription.lower() + \" \"\n",
        "\n",
        "  return transcript"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
            "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:423: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1396: FutureWarning: The class `Wav2Vec2ForMaskedLM` is deprecated. Please use `Wav2Vec2ForCTC` instead.\n",
            "  \"The class `Wav2Vec2ForMaskedLM` is deprecated. Please use `Wav2Vec2ForCTC` instead.\", FutureWarning\n",
            "Some weights of Wav2Vec2ForMaskedLM were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-NCm2XgzJJm"
      },
      "source": [
        "gradio_asr = gr.Interface(\n",
        "    fn=asr_transcribe,\n",
        "    title=\"Speech to Text with Wav2Vec Hugging Face\",\n",
        "    description=\"Upload an audio clip, and let Transcribe the word\",\n",
        "    inputs=gr.inputs.Audio(label=\"Upload Audio File\", type=\"file\"),\n",
        "    outputs=gr.outputs.Textbox(label=\"Auto-Transcription\"),\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "0IRACyxMzLim",
        "outputId": "740063b4-fc0e-4c5f-9259-728f4a3ed137"
      },
      "source": [
        "gradio_asr.launch()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "This share link will expire in 72 hours. If you need a permanent link, visit: https://gradio.app/introducing-hosted\n",
            "Running on External URL: https://27251.gradio.app\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://27251.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7feaa906ec50>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Flask 'gradio.networking'>,\n",
              " 'http://127.0.0.1:7860/',\n",
              " 'https://27251.gradio.app')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}